{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "public-cardiff",
   "metadata": {},
   "source": [
    "학습목표\n",
    "- 텍스트 데이터를 머신러닝 입출력용 수치데이터로 변환하는 과정을 이해한다.\n",
    "- RNN의 특징을 이해하고 시퀀셜한 데이터를 다루는 방법을 이해한다.\n",
    "- 1-D CNN으로도 텍스트를 처리할 수 있음을 이해한다.\n",
    "- IMDB와 네이버 영화리뷰 데이터셋을 이용한 영화리뷰 감성분류 실습을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-steel",
   "metadata": {},
   "source": [
    "##  텍스트를 숫자로 표현하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "duplicate-placement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thrown-facing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incident-resident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "# 우리가 가진 텍스트 데이터를 숫자로 바꿔보려고 하는데, 텍스트를 숫자로 바꾸려면 위의 딕셔너리가 {텍스트:인덱스}구조여야 함.\n",
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moving-costa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exposed-clearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n<BOS> -> 1, i -> 3, eat -> 6, lunch -> 7\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제 우리가 가진 텍스트 데이터를 숫자로 바꿔 표현해 봅시다.\n",
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))\n",
    "\"\"\"\n",
    "<BOS> -> 1, i -> 3, eat -> 6, lunch -> 7\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "geographic-cradle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mediterranean-vertical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# 반대로, encode된 벡터를 decode하여 다시 원래 텍스트 데이터로 복구할 수도 있습니다.\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "driving-perfume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-doctrine",
   "metadata": {},
   "source": [
    "# embedding 레이어의 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nasty-brief",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-677d98122cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# list 형태의 sentences는 numpy array로 변환되어야 딥러닝 레이어의 입력이 될 수 있습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mraw_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_encoded_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m     if any(isinstance(x, (\n\u001b[1;32m    959\u001b[0m         np_arrays.ndarray, np.ndarray, float, int)) for x in input_list):\n\u001b[0;32m--> 960\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_convert_numpy_or_python_types\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3307\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3308\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3310\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1405\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# embedding 레이어를 활용하여 이전 스텝의 텍스트 데이터를 워드 벡터 텐서 형태로 다시 표현\n",
    "\n",
    "# 아래 코드는 그대로 실행하시면 에러가 발생할 것입니다. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 위 그림과 같이 4차원의 워드 벡터를 가정합니다. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \n",
    "# list 형태의 sentences는 numpy array로 변환되어야 딥러닝 레이어의 입력이 될 수 있습니다.\n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index))\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-toolbox",
   "metadata": {},
   "source": [
    "위 코드를 실행하면 에러가 발생합니다. 이는 embedding 레이어의 인풋이 되는 문장 벡터는 그 길이가 일정해야 합니다. raw_inputs의 3개 벡터의 길이는 각각 4,4,5 입니다.\n",
    "\n",
    "tensorflow에서는 keras.preprocessing.sequence.pad_sequences라는 편리한 함수를 통해 문장 벡터 뒤에 패딩을 추가하여 길이를 일정하게 맞춰주는 기능을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "western-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 4 5 0]\n",
      " [1 3 6 7 0]\n",
      " [1 8 3 4 9]]\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "proper-fossil",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.01011405 -0.01326608 -0.0182943  -0.02537967]\n",
      "  [ 0.04415477  0.04008099 -0.04434595 -0.04613638]\n",
      "  [-0.01381892  0.02739409 -0.04793549 -0.01694747]\n",
      "  [-0.03792977 -0.01781761 -0.02282174 -0.02709488]\n",
      "  [ 0.0304256   0.02521263 -0.02966472 -0.03759736]]\n",
      "\n",
      " [[-0.01011405 -0.01326608 -0.0182943  -0.02537967]\n",
      "  [ 0.04415477  0.04008099 -0.04434595 -0.04613638]\n",
      "  [ 0.04864373 -0.01861961  0.02289558  0.01804664]\n",
      "  [-0.00638711 -0.02925316  0.04075861 -0.01692899]\n",
      "  [ 0.0304256   0.02521263 -0.02966472 -0.03759736]]\n",
      "\n",
      " [[-0.01011405 -0.01326608 -0.0182943  -0.02537967]\n",
      "  [ 0.03995342 -0.04976204 -0.01659255 -0.03087068]\n",
      "  [ 0.04415477  0.04008099 -0.04434595 -0.04613638]\n",
      "  [-0.01381892  0.02739409 -0.04793549 -0.01694747]\n",
      "  [-0.02817357  0.03456438 -0.04853749  0.01931543]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n shape=(3,5,4)에서 3은 입력 문장 개수, 5는 입력문장의 최대 길이, 4는 워드 벡터의 차원의 수\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 코드를 실행하고 나서 다시 output=embedding(raw_inputs)을 실행\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index))\n",
    "raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)\n",
    "\n",
    "\"\"\"\n",
    " shape=(3,5,4)에서 3은 입력 문장 개수, 5는 입력문장의 최대 길이, 4는 워드 벡터의 차원의 수\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-trial",
   "metadata": {},
   "source": [
    "# 시퀀스 데이터를 다루는 RNN\n",
    "RNN은 시간의 흐름에 따라 새롭게 들어오는 입력에 따라 변하는 현재 상태를 묘사하는 state machine으로 설계되었습니다. 이는 과거시점의 입력을 모두 저장하여 다음 시점으로 넘어가는 형태라, 각 시점의 입력들이 축척 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "posted-restriction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-cheat",
   "metadata": {},
   "source": [
    "# 꼭 RNN이어야 할까?\n",
    "텍스트를 처리하기 위해 RNN이 아니라 1-D Convolution Nerual Network를 사용할 수도 있습니다. 이 모델은 문장 전체를 한꺼번에 한 방향으로 길이 7짜리 필터로 스캐닝 하면서 7단어 이내에서 발견되는 특징을. 추출하여 그것으로 문장을 분류하는 방식. 또한 CNN 계열은 RNN 계열보다 병렬처리가 효율적이기 때문에 학습 속도도 훨씬 빠르게 진행된다는 장점이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "radical-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          464       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,457\n",
      "Trainable params: 2,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-liberia",
   "metadata": {},
   "source": [
    "GlobalMaxPooling1D() 레이어 하나만 사용하는 방법도 있습니다. 이 방식은 전체 문장 중에서 단 하나의 가장 중요한 단어만 피처로 추출하여 그것으로 문장의 긍정/부정을 평가하는 방식."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "foreign-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-earthquake",
   "metadata": {},
   "source": [
    "# IMDB 영화리뷰 감성분석\n",
    "긍정은 1, 부정은 0의 라벨이 달려있습니다. 50000개의 리뷰 중 절반인 25000개가 훈련용, 나머지가 테스트용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "charitable-saying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 분석\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-celebrity",
   "metadata": {},
   "source": [
    "`imdb.load_data()` 호출 시 단어사전에 등재할 단어의 개수(num_words)를 10000으로 지정하면, 그 개수만큼의 `word_to_index` 딕셔너리까지 생성된 형태로 데이터셋이 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "actual-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "serious-language",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 이미 encode가 되어있습니다.\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-discipline",
   "metadata": {},
   "source": [
    "IMDb 데이터셋의 텍스트 인코딩을 위한 `word_to_index`, `index_to_word`는 아래와 같이 보정되어야 합니다. 아래 내용은 Tensorflow 튜토리얼의 가이드를 반영하여 작성하였습니다.\n",
    "`word_to_index`는 IMDb 텍스트 데이터셋의 단어 출현 빈도 기준으로 내림차수 정렬되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "breeding-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "interesting-theater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "# encode된 텍스트가 정상적으로 decode되는지 확인해 보겠습니다.\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "environmental-toronto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "# pad_sequences를 통해 데이터셋 상의 문장의 길이를 통일. maxlen의 값 설정도 전체 모델 성능에 영향을 미칩니다.\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "strategic-donna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "# padding 방식이 post인지 pre인지에 따라서 전체적인 성능 차이가 있습니다.\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# pre가 훨씬 유리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-oriental",
   "metadata": {},
   "source": [
    "# 모델 설계와 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "contrary-doctor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-functionality",
   "metadata": {},
   "source": [
    "model 훈련 전에, 훈련용 데이터셋 25000건 중 10000건을 분리하여 검증셋(validation set)으로 사용하도록 합니다. 적절한 validation 데이터는 몇 개가 좋을지 고민해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bored-speed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "first-pontiac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 6s 99ms/step - loss: 0.6929 - accuracy: 0.5110 - val_loss: 0.6906 - val_accuracy: 0.6015\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.6846 - accuracy: 0.6270 - val_loss: 0.6646 - val_accuracy: 0.7163\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.6239 - accuracy: 0.7923 - val_loss: 0.4556 - val_accuracy: 0.8296\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.3651 - accuracy: 0.8705 - val_loss: 0.3233 - val_accuracy: 0.8617\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.2089 - accuracy: 0.9199 - val_loss: 0.3018 - val_accuracy: 0.8777\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.1441 - accuracy: 0.9558 - val_loss: 0.3197 - val_accuracy: 0.8750\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0984 - accuracy: 0.9727 - val_loss: 0.3511 - val_accuracy: 0.8679\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0708 - accuracy: 0.9833 - val_loss: 0.3944 - val_accuracy: 0.8661\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.0439 - accuracy: 0.9925 - val_loss: 0.4229 - val_accuracy: 0.8641\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.0316 - accuracy: 0.9969 - val_loss: 0.4527 - val_accuracy: 0.8614\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0230 - accuracy: 0.9982 - val_loss: 0.4943 - val_accuracy: 0.8605\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0173 - accuracy: 0.9987 - val_loss: 0.5186 - val_accuracy: 0.8591\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0108 - accuracy: 0.9993 - val_loss: 0.5478 - val_accuracy: 0.8584\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0088 - accuracy: 0.9994 - val_loss: 0.5839 - val_accuracy: 0.8571\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0062 - accuracy: 0.9997 - val_loss: 0.5914 - val_accuracy: 0.8553\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.0071 - accuracy: 0.9993 - val_loss: 0.6154 - val_accuracy: 0.8553\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 0.6355 - val_accuracy: 0.8541\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.8531\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6772 - val_accuracy: 0.8533\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6964 - val_accuracy: 0.8527\n"
     ]
    }
   ],
   "source": [
    "# model 학습을 시작해 봅시다.\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "recreational-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 3s - loss: 0.7519 - accuracy: 0.8421\n",
      "[0.7519068121910095, 0.8421199917793274]\n"
     ]
    }
   ],
   "source": [
    "# 학습이 끝난 모델을 테스트셋으로 평가해 봅니다.\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-commissioner",
   "metadata": {},
   "source": [
    "`model.fit()` 과정 중의 train/validation loss, accuracy 등이 매 epoch마다 history 변수에 저장되어 있습니다.\n",
    "이 데이터를 그래프로 그려 보면, 수행했던 딥러닝 학습이 잘 진행되었는지, 오버피팅 혹은 언더피팅하지 않았는지, 성능을 개선할 수 있는 다양한 아이디어를 얻을 수 있는 좋은 자료가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "received-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-blackjack",
   "metadata": {},
   "source": [
    "Training and validation loss를 그려 보면, 몇 epoch까지의 트레이닝이 적절한지 최적점을 추정해 볼 수 있습니다. validation loss의 그래프가 train loss와의 이격이 발생하게 되면 더 이상의 트레이닝은 무의미해지게 마련입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "numerical-rendering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-absolute",
   "metadata": {},
   "source": [
    "# Word2Vec의 적용\n",
    "사용했던 model의 첫 번째 레이어는 바로 Embedding 레이어였습니다. 이 레이어는 우리가 가진 사전의 단어 개수 X 워드 벡터 사이즈만큼의 크기를 가진 학습 파라미터였습니다. 만약 우리의 감성분류 모델이 학습이 잘 되었다면, Embedding 레이어에 학습된 우리의 워드 벡터들도 의미 공간상에 유의미한 형태로 학습되었을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "classified-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "smoking-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "scheduled-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.0304888 ,  0.00653872,  0.02157151,  0.01217426, -0.04007436,\n",
       "        0.00442104, -0.07201403,  0.01498448,  0.00517091,  0.0151215 ,\n",
       "        0.02762864,  0.00631357, -0.00762655,  0.0195368 ,  0.0173985 ,\n",
       "        0.01658303], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gensim에서 제공하는 패키지를 이용해, 위에 남긴 임베딩 파라미터를 읽어서 word vector로 활용할 수 있습니다.\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-scholarship",
   "metadata": {},
   "source": [
    " 워드 벡터가 의미벡터 공간상에 유의미하게 학습되었는지 확인하는 방법 중에, 단어를 하나 주고 그와 가장 유사한 단어와 그 유사도를 확인하는 방법이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ordinary-tiger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decidedly', 0.848314642906189),\n",
       " ('realise', 0.8212144374847412),\n",
       " ('thrilled', 0.8202900290489197),\n",
       " ('marvellous', 0.8192571997642517),\n",
       " ('siblings', 0.8073940277099609),\n",
       " ('universe', 0.8056557178497314),\n",
       " ('farm', 0.8041637539863586),\n",
       " ('future', 0.8007098436355591),\n",
       " ('humor', 0.7980731725692749),\n",
       " ('bravery', 0.7944832444190979)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-botswana",
   "metadata": {},
   "source": [
    "우리가 다룬 정도의 훈련데이터로는 워드 벡터를 정교하게 학습시키기 어렵습니다.이번에는 구글에서 제공하는 Word2Vec이라는 사전학습된(Pretrained) 워드 임베딩 모델을 가져다 활용해 보겠습니다. Word2Vec은 무려 1억 개의 단어로 구성된 Google News dataset을 바탕으로 학습되었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eight-garbage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-salon",
   "metadata": {},
   "source": [
    "300dim의 벡터로 이루어진 300만 개의 단어입니다. 이 단어 사전을 메모리에 모두 로딩하면 아주 높은 확률로 여러분의 실습환경에 메모리 에러가 날 것입니다. 그래서 KeyedVectors.load_word2vec_format 메소드로 워드 벡터를 로딩할 때 가장 많이 사용되는 상위 100만 개만 limt으로 조건을 주어 로딩했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "disturbed-monroe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907791495323181),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.661863386631012),\n",
       " ('passion', 0.6100708842277527),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547304749488831),\n",
       " ('absolutely_adore', 0.5536840558052063),\n",
       " ('adores', 0.5440906882286072)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-latitude",
   "metadata": {},
   "source": [
    "이제 우리는 이전 스텝에서 학습했던 모델의 임베딩 레이어를 Word2Vec의 것으로 교체하여 다시 학습시켜 볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "confidential-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "accepted-defense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 580, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 574, 16)           33616     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 114, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 108, 16)           1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,035,569\n",
      "Trainable params: 3,035,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원 수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "comic-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 14s 416ms/step - loss: 0.7057 - accuracy: 0.5031 - val_loss: 0.6917 - val_accuracy: 0.5190\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 11s 368ms/step - loss: 0.6821 - accuracy: 0.5598 - val_loss: 0.6759 - val_accuracy: 0.5862\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 11s 367ms/step - loss: 0.6517 - accuracy: 0.6504 - val_loss: 0.5993 - val_accuracy: 0.6975\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 11s 382ms/step - loss: 0.5163 - accuracy: 0.7848 - val_loss: 0.3773 - val_accuracy: 0.8396\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 11s 380ms/step - loss: 0.3092 - accuracy: 0.8782 - val_loss: 0.3110 - val_accuracy: 0.8684\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 11s 375ms/step - loss: 0.2242 - accuracy: 0.9157 - val_loss: 0.2980 - val_accuracy: 0.8758\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 11s 378ms/step - loss: 0.1697 - accuracy: 0.9411 - val_loss: 0.2928 - val_accuracy: 0.8789\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.1233 - accuracy: 0.9616 - val_loss: 0.3684 - val_accuracy: 0.8547\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.1129 - accuracy: 0.9625 - val_loss: 0.3091 - val_accuracy: 0.8768\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 11s 373ms/step - loss: 0.0752 - accuracy: 0.9832 - val_loss: 0.3226 - val_accuracy: 0.8778\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 11s 375ms/step - loss: 0.0553 - accuracy: 0.9911 - val_loss: 0.3383 - val_accuracy: 0.8753\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 11s 382ms/step - loss: 0.0406 - accuracy: 0.9950 - val_loss: 0.3646 - val_accuracy: 0.8754\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 11s 369ms/step - loss: 0.0309 - accuracy: 0.9970 - val_loss: 0.3749 - val_accuracy: 0.8758\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 11s 367ms/step - loss: 0.0265 - accuracy: 0.9978 - val_loss: 0.3984 - val_accuracy: 0.8738\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 11s 368ms/step - loss: 0.0168 - accuracy: 0.9993 - val_loss: 0.4098 - val_accuracy: 0.8753\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 11s 372ms/step - loss: 0.0145 - accuracy: 0.9992 - val_loss: 0.4241 - val_accuracy: 0.8749\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 11s 366ms/step - loss: 0.0103 - accuracy: 0.9995 - val_loss: 0.4385 - val_accuracy: 0.8747\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 11s 367ms/step - loss: 0.0082 - accuracy: 0.9995 - val_loss: 0.4539 - val_accuracy: 0.8741\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 11s 374ms/step - loss: 0.0068 - accuracy: 0.9996 - val_loss: 0.4646 - val_accuracy: 0.8743\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 11s 367ms/step - loss: 0.0050 - accuracy: 0.9998 - val_loss: 0.4769 - val_accuracy: 0.8732\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "unlimited-blues",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 7s - loss: 0.5145 - accuracy: 0.8651\n",
      "[0.5145267248153687, 0.865119993686676]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-toyota",
   "metadata": {},
   "source": [
    "# 프로젝트\n",
    "\n",
    "순서)\n",
    "1. 데이터 준비와 확인\n",
    "2. 데이터로더 구성\n",
    "3. 모델구성을 위한 데이터 분석 및 가공\n",
    "4. 모델구성 및 validation set 구성\n",
    "5. 모델 훈련 개시\n",
    "6. Loss, Accuracy 그래프 시각화\n",
    "7. 학습된 Embedding 레이어 분석\n",
    "8. 한국어 Word2Vec 임베딩 활용하여 성능개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "amber-enlargement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-hello",
   "metadata": {},
   "source": [
    "실습 때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 imdb.data_loader() 메소드를 호출하면 숫자 인덱스로 변환된 텍스트와 word_to_index 딕셔너리까지 친절하게 제공합니다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있습니다. 이것을 읽어서 imdb.data_loader()와 동일하게 동작하는 자신만의 data_loader를 만들어 보는 것으로 시작합니다. data_loader 안에서는 다음을 수행해야 합니다.\n",
    "\n",
    "- 데이터의 중복 제거\n",
    "- NaN 결측치 제거\n",
    "- 한국어 토크나이저로 토큰화\n",
    "- 불용어(Stopwords) 제거\n",
    "- 사전word_to_index 구성\n",
    "- 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "- X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "intellectual-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "prospective-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "sudden-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "registered-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "# 3.\n",
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "joined-marking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 41)\n"
     ]
    }
   ],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "appropriate-spending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49157, 41)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "great-explanation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146182"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fancy-click",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126182, 41)\n",
      "(126182,)\n",
      "(20000, 41)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "4. \n",
    "# validation set 20000건 분리\n",
    "x_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "# validation set을 제외한 나머지\n",
    "partial_X_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-dollar",
   "metadata": {},
   "source": [
    "같은 파라미터로 3가지 모델을 학습\n",
    "- 1-D CNN\n",
    "- LSTM\n",
    "- GlobalMaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "responsible-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, None, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 256)         537856    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, None, 64)          114752    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,653,137\n",
      "Trainable params: 3,653,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-D CNN\n",
    "CNN_model = keras.Sequential(name=\"CNN\")\n",
    "CNN_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "CNN_model.add(keras.layers.Conv1D(256, 7, activation='relu'))\n",
    "CNN_model.add(keras.layers.MaxPooling1D(5))\n",
    "CNN_model.add(keras.layers.Conv1D(64, 7, activation='relu'))\n",
    "CNN_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "CNN_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "CNN_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "prostate-discount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, None, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 9888      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,009,969\n",
      "Trainable params: 3,009,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "LSTM_model = keras.Sequential(name=\"LSTM\")\n",
    "LSTM_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "LSTM_model.add(keras.layers.LSTM(8, dropout=0.7))\n",
    "LSTM_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "LSTM_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "flying-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GMP\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, None, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 8)                 2408      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,002,417\n",
      "Trainable params: 3,002,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GlobalMaxPooling1D\n",
    "\n",
    "GMP_model = keras.Sequential(name=\"GMP\")\n",
    "GMP_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "GMP_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "GMP_model.add(keras.layers.Dense(8, activation='relu'))\n",
    "GMP_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "GMP_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "innovative-ecuador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNN', 'LSTM', 'GMP']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lst = [CNN_model.name, LSTM_model.name, GMP_model.name]\n",
    "model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "moved-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Start fitting CNN ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 25s 89ms/step - loss: 0.6142 - accuracy: 0.6624 - val_loss: 0.4755 - val_accuracy: 0.8448\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 19s 77ms/step - loss: 0.3862 - accuracy: 0.8673 - val_loss: 0.3173 - val_accuracy: 0.8622\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 19s 75ms/step - loss: 0.2282 - accuracy: 0.9113 - val_loss: 0.3382 - val_accuracy: 0.8605\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 19s 75ms/step - loss: 0.1453 - accuracy: 0.9491 - val_loss: 0.4002 - val_accuracy: 0.8525\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 19s 75ms/step - loss: 0.0900 - accuracy: 0.9714 - val_loss: 0.4997 - val_accuracy: 0.8518\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 19s 75ms/step - loss: 0.0574 - accuracy: 0.9825 - val_loss: 0.6029 - val_accuracy: 0.8475\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 19s 76ms/step - loss: 0.0406 - accuracy: 0.9885 - val_loss: 0.7004 - val_accuracy: 0.8452\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 18s 75ms/step - loss: 0.0313 - accuracy: 0.9912 - val_loss: 0.7530 - val_accuracy: 0.8471\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 18s 75ms/step - loss: 0.0276 - accuracy: 0.9924 - val_loss: 0.8267 - val_accuracy: 0.8385\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 18s 74ms/step - loss: 0.0300 - accuracy: 0.9908 - val_loss: 0.9179 - val_accuracy: 0.8431\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 19s 77ms/step - loss: 0.0338 - accuracy: 0.9897 - val_loss: 0.8635 - val_accuracy: 0.8444\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 18s 75ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.9049 - val_accuracy: 0.8417\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 18s 74ms/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 1.0007 - val_accuracy: 0.8405\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 19s 78ms/step - loss: 0.0209 - accuracy: 0.9934 - val_loss: 1.0520 - val_accuracy: 0.8421\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 19s 76ms/step - loss: 0.0189 - accuracy: 0.9944 - val_loss: 1.0497 - val_accuracy: 0.8408\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 19s 77ms/step - loss: 0.0239 - accuracy: 0.9925 - val_loss: 1.0643 - val_accuracy: 0.8426\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 19s 77ms/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 1.2045 - val_accuracy: 0.8414\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 19s 75ms/step - loss: 0.0162 - accuracy: 0.9945 - val_loss: 1.1802 - val_accuracy: 0.8393\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 19s 78ms/step - loss: 0.0167 - accuracy: 0.9940 - val_loss: 1.3271 - val_accuracy: 0.8397\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 18s 74ms/step - loss: 0.0184 - accuracy: 0.9930 - val_loss: 1.2501 - val_accuracy: 0.8415\n",
      "Start evaluating CNN ...\n",
      "1537/1537 - 5s - loss: 1.2834 - accuracy: 0.8394\n",
      "----------------------------------------\n",
      "Start fitting LSTM ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 11s 40ms/step - loss: 0.5602 - accuracy: 0.7255 - val_loss: 0.3577 - val_accuracy: 0.8472\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 9s 36ms/step - loss: 0.3430 - accuracy: 0.8570 - val_loss: 0.3372 - val_accuracy: 0.8555\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.3123 - accuracy: 0.8699 - val_loss: 0.3328 - val_accuracy: 0.8562\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.2943 - accuracy: 0.8773 - val_loss: 0.3320 - val_accuracy: 0.8577\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.2793 - accuracy: 0.8840 - val_loss: 0.3349 - val_accuracy: 0.8573\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 9s 36ms/step - loss: 0.2662 - accuracy: 0.8893 - val_loss: 0.3356 - val_accuracy: 0.8569\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 9s 36ms/step - loss: 0.2573 - accuracy: 0.8930 - val_loss: 0.3420 - val_accuracy: 0.8578\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.2473 - accuracy: 0.8979 - val_loss: 0.3463 - val_accuracy: 0.8577\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.2381 - accuracy: 0.9019 - val_loss: 0.3485 - val_accuracy: 0.8569\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.2313 - accuracy: 0.9063 - val_loss: 0.3554 - val_accuracy: 0.8591\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.2213 - accuracy: 0.9107 - val_loss: 0.3589 - val_accuracy: 0.8580\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 9s 36ms/step - loss: 0.2159 - accuracy: 0.9122 - val_loss: 0.3669 - val_accuracy: 0.8578\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 9s 37ms/step - loss: 0.2134 - accuracy: 0.9131 - val_loss: 0.3698 - val_accuracy: 0.8571\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.2053 - accuracy: 0.9161 - val_loss: 0.3792 - val_accuracy: 0.8551\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.2016 - accuracy: 0.9191 - val_loss: 0.3873 - val_accuracy: 0.8548\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 9s 35ms/step - loss: 0.1981 - accuracy: 0.9198 - val_loss: 0.3914 - val_accuracy: 0.8554\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 9s 36ms/step - loss: 0.1925 - accuracy: 0.9222 - val_loss: 0.4019 - val_accuracy: 0.8511\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 9s 37ms/step - loss: 0.1902 - accuracy: 0.9234 - val_loss: 0.4038 - val_accuracy: 0.8528\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 9s 38ms/step - loss: 0.1845 - accuracy: 0.9254 - val_loss: 0.4086 - val_accuracy: 0.8526\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 9s 36ms/step - loss: 0.1819 - accuracy: 0.9283 - val_loss: 0.4125 - val_accuracy: 0.8525\n",
      "Start evaluating LSTM ...\n",
      "1537/1537 - 5s - loss: 0.4123 - accuracy: 0.8496\n",
      "----------------------------------------\n",
      "Start fitting GMP ...\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 8s 29ms/step - loss: 0.5693 - accuracy: 0.7311 - val_loss: 0.3428 - val_accuracy: 0.8486\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 7s 28ms/step - loss: 0.3090 - accuracy: 0.8707 - val_loss: 0.3308 - val_accuracy: 0.8561\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.2545 - accuracy: 0.8986 - val_loss: 0.3364 - val_accuracy: 0.8566\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.2114 - accuracy: 0.9197 - val_loss: 0.3541 - val_accuracy: 0.8536\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.1653 - accuracy: 0.9422 - val_loss: 0.3769 - val_accuracy: 0.8553\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.1217 - accuracy: 0.9618 - val_loss: 0.4169 - val_accuracy: 0.8506\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.0842 - accuracy: 0.9768 - val_loss: 0.4541 - val_accuracy: 0.8471\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.0561 - accuracy: 0.9862 - val_loss: 0.4938 - val_accuracy: 0.8469\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 7s 28ms/step - loss: 0.0366 - accuracy: 0.9915 - val_loss: 0.5359 - val_accuracy: 0.8461\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 7s 28ms/step - loss: 0.0245 - accuracy: 0.9944 - val_loss: 0.5752 - val_accuracy: 0.8457\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.0185 - accuracy: 0.9955 - val_loss: 0.6102 - val_accuracy: 0.8430\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.0146 - accuracy: 0.9960 - val_loss: 0.6399 - val_accuracy: 0.8446\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.6637 - val_accuracy: 0.8440\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.6913 - val_accuracy: 0.8445\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 7s 28ms/step - loss: 0.0090 - accuracy: 0.9967 - val_loss: 0.7122 - val_accuracy: 0.8440\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 7s 28ms/step - loss: 0.0085 - accuracy: 0.9967 - val_loss: 0.7378 - val_accuracy: 0.8439\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 7s 28ms/step - loss: 0.0075 - accuracy: 0.9968 - val_loss: 0.7571 - val_accuracy: 0.8447\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 7s 28ms/step - loss: 0.0071 - accuracy: 0.9969 - val_loss: 0.7777 - val_accuracy: 0.8439\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.0063 - accuracy: 0.9973 - val_loss: 0.7989 - val_accuracy: 0.8438\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 7s 29ms/step - loss: 0.0065 - accuracy: 0.9973 - val_loss: 0.8138 - val_accuracy: 0.8438\n",
      "Start evaluating GMP ...\n",
      "1537/1537 - 2s - loss: 0.8232 - accuracy: 0.8410\n"
     ]
    }
   ],
   "source": [
    "model_result = {}\n",
    "\n",
    "for model_name in model_lst:\n",
    "    \n",
    "    if model_name == \"CNN\":\n",
    "        model = CNN_model\n",
    "    elif model_name == \"LSTM\":\n",
    "        model = LSTM_model\n",
    "    else :\n",
    "        model = GMP_model\n",
    "    \n",
    "    print('-'*40)\n",
    "    print(\"Start fitting {} ...\".format(model_name))\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    epochs=20\n",
    "\n",
    "    history = model.fit(partial_X_train,\n",
    "                        partial_y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=1)\n",
    "    \n",
    "    \n",
    "    print(\"Start evaluating {} ...\".format(model_name))\n",
    "    results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "    model_result[model_name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "charged-cloud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM \t 0.8495839834213257\n",
      "GMP \t 0.8410399556159973\n",
      "CNN \t 0.8393514752388\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "for name, [_, acc] in sorted(model_result.items(), key=lambda x : x[1][1], reverse=True) :\n",
    "    print(name,'\\t',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-commonwealth",
   "metadata": {},
   "source": [
    "3가지 모델중 LSTM이 가장 좋은 점수를 냈습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "verified-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ambient-worth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "986/986 [==============================] - 75s 74ms/step - loss: 0.4570 - accuracy: 0.7762 - val_loss: 0.3400 - val_accuracy: 0.8537\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85370, saving model to model.h5\n",
      "Epoch 2/10\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.3088 - accuracy: 0.8676 - val_loss: 0.3178 - val_accuracy: 0.8639\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.85370 to 0.86390, saving model to model.h5\n",
      "Epoch 3/10\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.2773 - accuracy: 0.8821 - val_loss: 0.3132 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.86390 to 0.86695, saving model to model.h5\n",
      "Epoch 4/10\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.2472 - accuracy: 0.8960 - val_loss: 0.3138 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.86695\n",
      "Epoch 5/10\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.2250 - accuracy: 0.9068 - val_loss: 0.3275 - val_accuracy: 0.8679\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.86695 to 0.86790, saving model to model.h5\n",
      "Epoch 6/10\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.2043 - accuracy: 0.9167 - val_loss: 0.3346 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.86790\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 1000  # 워드 벡터의 차원수\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "\n",
    "model.add(keras.layers.LSTM(128, dropout=0.7))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 이진분류\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                   callbacks=[early_stopping, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "harmful-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 7s - loss: 0.3407 - accuracy: 0.8624\n",
      "[0.3406710624694824, 0.8623797297477722]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "blind-sodium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "neutral-environment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# 빨간 실선으로 표시\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# 파란 실선으로 표시\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eastern-thinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# 7.\n",
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "satisfied-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장\n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim)) # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀작성\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록\n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "understanding-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['대박']\n",
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "moving-pattern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('최고', 0.5585905313491821),\n",
       " ('수준급', 0.5581956505775452),\n",
       " ('후딱', 0.5467694997787476),\n",
       " ('울컥', 0.5409130454063416),\n",
       " ('소장', 0.5390514135360718),\n",
       " ('탄탄', 0.5389233231544495),\n",
       " ('먹먹', 0.5387176275253296),\n",
       " ('담백', 0.5386830568313599),\n",
       " ('올려야', 0.5385977625846863),\n",
       " ('손색없', 0.5373140573501587)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"대박\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-quilt",
   "metadata": {},
   "source": [
    "# 회고\n",
    "https://github.com/tyshin94/MODULABS_AIFFEL-Course/blob/fa6d83986edec2e389f552559b453543ae0665ed/Mini%20Project/Sentiment%20classification/2020.09.01.%20Tue%20naver_movie_review.ipynb 의 코드를 참고해서 작성했습니다.\n",
    "\n",
    "지금까지 아이펠의 교육과정은 대부분이 computer vision의 관련된 수업이었습니다. 그래서 아직은 nlp가 무척 어색하고 어렵습니다.... 그래서 제가 한 부분까지는 그래도 쪼오오오끔이나마 이해가 가능한데 한국어 word2vec은 당체 이해가 안되서 못했습니다. 별점도 항상 3개씩 받고는 싶지만, 아직 저는 평가를 받을수 있는 수준은 함참도 안되고 공부하고 배운다는 마음가짐으로 제가 이해한 부분만 올렸습니다. 제가 한 부분도 사실 참고한 코드를 복붙한 부분이 대부분이라 제가 한 작업이라고 하기도 힘들긴 하지만요. 변명아닌 변명을 하자면.... 노드가 밀려있어서 exploration 노드를 끝내기도 힘든데 밀린 노드까지 하려고 하니 시간이 많이 부족하고 체력적인 한계에 여기까지 했습니다.. 죄송합니다... 다음에는 루브릭의 조건을 다 충족할 수 있게 더 노력하겠습니다!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-humidity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
